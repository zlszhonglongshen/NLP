{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "# 准备工作\n",
    "#\n",
    "# 1、训练好的词向量\n",
    "#\n",
    "# 2、用于训练的文本（已完成分词，每篇文章且含有对应label）\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000 # 每篇文章选取1000个词\n",
    "MAX_NB_WORDS = 10000 # 将字典设置为含有1万个词\n",
    "EMBEDDING_DIM = 300 # 词向量维度，300维\n",
    "VALIDATION_SPLIT = 0.2 # 测试集大小，全部数据的20%\n",
    "\n",
    "# step 1 选取词频最高的一部分词\n",
    "#\n",
    "# 预训练好的词向量200万个词每个300维，这个脚本的目的是实验性的将流程跑通。模型训练过程没问题后再增加词的个数。\n",
    "\n",
    "# 目的是得到一份字典(embeddings_index)含有1万个词，每个词对应属于自己的300维向量\n",
    "embeddings_index = {}\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "path = '../word2vec_model'\n",
    "model = gensim.models.Word2Vec.load(path)\n",
    "word_vectors = model.wv\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "    if int(vocab_obj.index) < MAX_NB_WORDS:\n",
    "        embeddings_index[word] = word_vectors[word]\n",
    "del model, word_vectors # 删掉gensim模型释放内存\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# print out:\n",
    "# Indexing word vectors.\n",
    "# Found 10000 word vectors.\n",
    "\n",
    "# step 2 获取训练文本和对应的标签\n",
    "#\n",
    "# 我的训练数据保存成了csv文件，有三列 content, channel_id, name，其中的name与channel_id是一一对应的。content已经提前分好词。\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels = []  # list of label ids\n",
    "labels_index = {}  # label与name的对应关系\n",
    "\n",
    "# 读取数据\n",
    "path = '../content.csv'\n",
    "contents = pd.read_csv(path)\n",
    "contents = contents.dropna()\n",
    "\n",
    "# 提取文本内容与label\n",
    "texts = contents['content'].values.tolist()\n",
    "labels = contents['channel_id'].map(int)\n",
    "labels = labels.values.tolist()\n",
    "\n",
    "# 获得label与name的对应关系\n",
    "tem_labels_index = contents.groupby(['name', 'channel_id']).size().reset_index()\n",
    "tem_labels_index = tem_labels_index[['channel_id', 'name']].values.tolist()\n",
    "for idx, name in tem_labels_index:\n",
    "    labels_index[name] = idx\n",
    "del contents, tem_labels_index\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# print out\n",
    "# Processing text dataset\n",
    "# Found 57867 texts.\n",
    "\n",
    "# step 3\n",
    "#\n",
    "# 文本准备，keras相关函数在keras 文档 Text Preprocessing 部分 可以找到\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # 传入我们词向量的字典\n",
    "tokenizer.fit_on_texts(texts) # 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "sequences = tokenizer.texts_to_sequences(texts) # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) # 限制每篇文章的长度\n",
    "\n",
    "labels = to_categorical(np.asarray(labels)) # label one hot表示\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# print out\n",
    "# Found 379653 unique tokens.\n",
    "# Shape of data tensor: (57867, 1000)\n",
    "# Shape of label tensor: (57867, 26) # 我的文本类别有26类\n",
    "\n",
    "# step 4 准备训练集与测试集\n",
    "\n",
    "# 打乱文章顺序\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "# 切割数据\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "# step 5 准备embedding layer\n",
    "\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))  # 对比词向量字典中包含词的个数与文本数据所有词的个数，取小\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # 文本数据中的词在词向量字典中没有，向量为取0；如果有则取词向量中该词的向量\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 将预训练好的词向量加载如embedding layer\n",
    "# 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "# step 6 训练模型\n",
    "#\n",
    "# 做了那么多准备，我们终于可以训练模型啦！\n",
    "#\n",
    "# keras 文档 pooling 部分\n",
    "#\n",
    "# keras 文档 convolutional 部分\n",
    "\n",
    "# 训练  1D 卷积神经网络 使用 Maxpooling1D\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(filters=128, kernel_size=5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D((pool_size=5)(x)\n",
    "x = Conv1D(filters=128, kernel_size=5, activation='relu')(x)\n",
    "x = MaxPooling1D((pool_size=5)(x)\n",
    "x = Conv1D(filters=128, kernel_size=5,, activation='relu')(x)\n",
    "x = MaxPooling1D((pool_size=35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# 如果希望短一些时间可以，epochs调小\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 总结\n",
    "#\n",
    "# 训练集准确率92.29%左右，测试集准确率74.96%左右，说明模型可能过拟合了。没关系，我们已经实现了目标。整个流程跑通了。为了提高准确率，可以尝试：\n",
    "#\n",
    "# 1、增加文章数量，这次测试我用的文章不多\n",
    "#\n",
    "# 2、文章类别均衡些，这次我用的文章类别严重有偏，某些类别文章特别多\n",
    "#\n",
    "# 3、尝试dropout和Batch normalization控制过拟合\n",
    "#\n",
    "# 4、尝试改变网络结构\n",
    "#\n",
    "#\n",
    "# 作者：斯坦因和他的狗\n",
    "# 链接：http://www.jianshu.com/p/7eed068ff353\n",
    "# 來源：简书\n",
    "# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
