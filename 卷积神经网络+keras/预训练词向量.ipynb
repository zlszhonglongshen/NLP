{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "'''\n",
    "压缩包提取指定后缀文件\n",
    "'''\n",
    "import re\n",
    "import rarfile\n",
    "import os\n",
    "\n",
    "def unrar_extract_txt(rar_path, save_path, reg='\\.txt|\\.TXT'):\n",
    "    \"\"\"\n",
    "    uncompressed rar files and extract txt\n",
    "    rar_path : where rar file exist\n",
    "    save_path : save extracted files\n",
    "    reg : for re.complie use\n",
    "    \"\"\"\n",
    "    regex = re.compile(reg)\n",
    "    for fname in os.listdir(rar_path):\n",
    "        rf = rarfile.RarFile(rar_path+fname)\n",
    "        for f in rf.infolist():\n",
    "            if regex.search(f.filename):\n",
    "                rf.extract(f, save_path)\n",
    "\n",
    "'''大文件切割'''\n",
    "\n",
    "def slice_preprocessing(file, save_path, chars=1e7):\n",
    "   \"\"\"\n",
    "   slice one big utf-8 file into small file\n",
    "   file : big files' path\n",
    "   save_path : directory to save small files\n",
    "   chars : chars numbers, each small file contains\n",
    "   \"\"\"\n",
    "   f = open(file, 'r')\n",
    "   data = f.read()\n",
    "   f.close()\n",
    "   data = data.decode('utf8','ignore')\n",
    "   data_l = len(data)\n",
    "   iters = int(data_l / chars) + 1\n",
    "   for n in range(iters):\n",
    "       start = int(n*chars)\n",
    "       end = int((1+n)*chars)\n",
    "       if end > data_l:\n",
    "           end = data_l\n",
    "       tem = data[start:end]\n",
    "       tem = tem.encode('utf8')\n",
    "\n",
    "       small_filename = save_path+os.path.split(file)[1]+'_'+str(start)+'_'+str(end)\n",
    "       f = open(small_filename, 'w')\n",
    "       f.write(tem)\n",
    "       f.close()\n",
    "\n",
    "#遍历文件目录下文件，且改名\n",
    "def prepare_files(search_path):\n",
    "    \"\"\" search_path : dir where file exist  \"\"\"\n",
    "    regex = re.compile('[\\.txt*|\\.TXT]')\n",
    "    file_count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(search_path, topdown=True):\n",
    "        for name in files:\n",
    "            if regex.search(name):\n",
    "                try: # shutil.copy() os.remove()\n",
    "                    new_name = '%d.txt' % (file_count)\n",
    "                    os.rename(os.path.join(root, name), os.path.join(root, new_name))\n",
    "                    file_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "    print('total renamed files : %d' % (file_count))\n",
    "    return file_count\n",
    "\n",
    "#文件编码检测，且预处理\n",
    "import jieba, re, random, chardet\n",
    "import numpy as np\n",
    "import logging\n",
    "import pool\n",
    "import time\n",
    "def processing(file_path):\n",
    "    \"\"\"\n",
    "    detect encoding and switch into unicode, remove some useless chars, slice context into numpy array\n",
    "    file_path : dir where file exist\n",
    "    return :\n",
    "    detect/switch/remove/slice success: (file name, ndarray(N, 1e4))\n",
    "    eg. for 23222.txt ('23222', ndarray(37, 1e4))\n",
    "    any step fail : return (-1, file name)\n",
    "    each element of numpy array is a word\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as txt_f:\n",
    "        context = txt_f.read()\n",
    "        con_len = len(context)\n",
    "\n",
    "    # if detect_coding['confidence'] <= 0.95, keep trying (max 5 times)\n",
    "    for n in range(5):\n",
    "        try:  # if length less then 1000 ignore this file\n",
    "            rand_num = random.randint(0, con_len - 1000)\n",
    "            detect_coding = chardet.detect(context[rand_num:rand_num + 1000])\n",
    "        except Exception as e:\n",
    "            return (-1, os.path.split(file_path)[1][:-4])\n",
    "\n",
    "        if detect_coding['confidence'] < 0.95:\n",
    "            if n == 4:\n",
    "                return (-1, os.path.split(file_path)[1][:-4])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if detect_coding['encoding'] == 'BG2312':\n",
    "                detect_coding['encoding'] = 'GBK'\n",
    "\n",
    "        # ignore illegal chars\n",
    "        context = context.decode(detect_coding['encoding'], 'ignore')\n",
    "\n",
    "        # replace useless chars\n",
    "        context = context.replace(' ', '')\n",
    "        context = context.replace('\\n', '')\n",
    "        context = context.replace('\\r', '')\n",
    "        context = context.replace(u'\\u3000',\n",
    "                                  '')  # u'\\u3000' 和 '\\xe3\\x80\\x80' is the same character  '　' looks like space but a little longer then space\n",
    "        context = re.sub('\\d+', '', context)\n",
    "        context = re.sub('[a-zA-z]', '', context)\n",
    "        context = re.sub(u'[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', '', context)\n",
    "\n",
    "        # jieba cut , and return a list contain utf8 words\n",
    "        context_cut = jieba.cut(context)\n",
    "        context_list = list(context_cut)\n",
    "        context_list = ' '.join(context_list).encode('utf8').split()\n",
    "\n",
    "        # prepare numpy array shape (N, 10000)\n",
    "        context_len = len(context_list)\n",
    "        N = context_len // 10000\n",
    "\n",
    "        if N == 0:  # if N less then 10000 ignore this file\n",
    "            return (-1, os.path.split(file_path)[1][:-4])\n",
    "        context_list = context_list[:N * 10000]\n",
    "        context_array = np.array(context_list).reshape((N, 10000))\n",
    "\n",
    "        return (os.path.split(file_path)[1][:-4], context_array)\n",
    "\n",
    "#多进程，完成数据预处理\n",
    "#如果一次多进程处理，内存会不够用，所以完成一个generator，将文件一批一批的feed给预处理函数，\n",
    "#多次使用多线程处理，数据处理完成后最好保持在hdf5文件中，总大小将近500G\n",
    "# 多进程pool.map(prepare_data, files)中的prepare_data函数必须是在顶层函数，且只接受一个参数(试验过lambda 也无法使用)\n",
    "def multi_processing_data(files_path_feeder, save_path, file_count, batch_size=5000):\n",
    "    \"\"\"\n",
    "    multi-processing, execute prepare_data(), save output into hdf5\n",
    "    files_path_feeder: a generator return files path by batch\n",
    "    save_path: hdf5 file path like ' ./output.hdf5'\n",
    "    file_count: total files to be prepare\n",
    "    batch_size: how many files to be prepared once\n",
    "    \"\"\"\n",
    "    ck_num = int(file_count / batch_size)\n",
    "    iter_times = 0\n",
    "    rows = 0\n",
    "    illegal_files = 0\n",
    "\n",
    "    start_p = time.time()\n",
    "    logging.info('start prepare_data')\n",
    "    logging.info('-------------------------------------------------------------')\n",
    "    for files in files_path_feeder:\n",
    "        start_l = time.time()\n",
    "        pool = Pool(45)\n",
    "        output = pool.map(prepare_data, files)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        illegal_files += len([n for n in output if n[0] == -1])  # count illegal files\n",
    "        output = [n for n in output if n[0] != -1]  # drop illegal_file\n",
    "        for n in output:  # count rows of corpus\n",
    "            rows += n[1].shape[0]\n",
    "\n",
    "        # write into hdf5\n",
    "        output = dict(output)\n",
    "        f = h5py.File(save_path, 'a')\n",
    "        for key, value in output.iteritems():\n",
    "            f.create_dataset(key, data=value)\n",
    "        f.close()\n",
    "        del output\n",
    "\n",
    "        # monitor processing\n",
    "        percentage = (iter_times + 1) / (ck_num + 1)\n",
    "        done = int(100 * percentage)\n",
    "        undone = 100 - done\n",
    "        iter_times += 1\n",
    "        logging.info('iteration %d th, multi-processing time: %0.2f s' % (iter_times, time.time() - start_l))\n",
    "        logging.info(''.join(['#'] * done + ['.'] * undone) + (' %0.2f' % (percentage * 100)) + '%')\n",
    "\n",
    "    logging.info('-------------------------------------------------------------')\n",
    "    logging.info('total files %d , illegal %d, effective %d (%0.2f) ' % (\n",
    "        file_count, illegal_files, file_count - illegal_files,\n",
    "        (file_count - illegal_files) / file_count))\n",
    "    logging.info('total rows %d , each row contains 10000 word(coding utf-8)' % (rows))\n",
    "    logging.info('done prepare_data, processing time: %0.2f s' % (time.time() - start_p))\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, size=300, alpha=0.025, window=5, min_count=10, max_vocab_size=None, sample=1e-3, seed=1, workers=45, min_alpha=0.0001, sg=0, hs=0, negative=20, cbow_mean=1, hashfxn=hash, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=1e4)\n",
    "\n",
    "\n",
    "###模型的保存和加载\n",
    "from gensim.keyedvectors import KeyedVectors\n",
    "# save\n",
    "model.save(fname) # 只有这样存才能继续训练!\n",
    "model.wv.save_word2vec_format(outfile + '.model.bin', binary=True)  # C binary format 磁盘空间比上一方法减半\n",
    "model.wv.save_word2vec_format(outfile + '.model.txt', binary=False) # C text format 磁盘空间大，与方法一样\n",
    "\n",
    "# load\n",
    "model = gensim.models.Word2Vec.load(fname)\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)\n",
    "\n",
    "# 最省内存的加载方法\n",
    "model = gensim.models.Word2Vec.load('model path')\n",
    "word_vectors = model.wv\n",
    "del model\n",
    "word_vectors.init_sims(replace=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
