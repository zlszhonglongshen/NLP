{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    f = open(filename,'r')\n",
    "    file_read = f.read()\n",
    "    words_=re.sub(\"[^a-zA-Z]+\", \" \",file_read).lower() #正则匹配，只留下单词，且大写改小写\n",
    "    words = list(words_.split()) #\n",
    "    return words\n",
    "\n",
    "words = read_file('哈利波特1-7英文原版.txt')\n",
    "\n",
    "vocabulary_size = 2000 #预定义频繁单词库的长度\n",
    "\n",
    "count = [['UNK',-1]] #初始化单词频数统计集合\n",
    "\n",
    "'''\n",
    "1.给@param “words”中出现过的单词做频数统计，取top 1999频数的单词放入dictionary中，以便快速查询。\n",
    "2.给哈利波特这本“单词库”@param“words”编码，出现在top 1999之外的单词，统一令其为“UNK”（未知），编号为0，并统计这些单词的数量。\n",
    "@return: 哈利波特这本书的编码data，每个单词的频数统计count，词汇表dictionary及其反转形式reverse_dictionary\n",
    "'''\n",
    "def build_dataset(words):\n",
    "    counter=collections.Counter(words).most_common(vocabulary_size-1) #length of all counter:22159 取top1999频数的单词作为vocabulary，其他的作为unknown\n",
    "    count.extend(counter)\n",
    "    #搭建dictionary\n",
    "    dictionary={}\n",
    "    for word,_ in count:\n",
    "        dictionary[word]=len(dictionary)\n",
    "    data=[]\n",
    "    #全部单词转为编号\n",
    "    #先判断这个单词是否出现在dictionary，如果是，就转成编号，如果不是，则转为编号0（代表UNK）\n",
    "    unk_count=0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index=dictionary[word]\n",
    "        else:\n",
    "            index=0\n",
    "            unk_count+=1\n",
    "        data.append(index)\n",
    "    count[0][1]=unk_count\n",
    "    reverse_dictionary=dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    return data,count,dictionary,reverse_dictionary\n",
    "\n",
    "data,count,dictionary,reverse_dictionary=build_dataset(words)   \n",
    "del words #删除原始单词列表，节约内存\n",
    "\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "'''\n",
    "采用Skip-Gram模式\n",
    "生成word2vec训练样本\n",
    "@param batch_size:每个批次训练多少样本\n",
    "@param num_skips: 为每个单词生成多少样本（本次实验是2个），batch_size必须是num_skips的整数倍,这样可以确保由一个目标词汇生成的样本在同一个批次中。\n",
    "@param skip_window:单词最远可以联系的距离（本次实验设为1，即目标单词只能和相邻的两个单词生成样本），2*skip_window>=num_skips\n",
    "'''\n",
    "def generate_batch(batch_size,num_skips,skip_window):\n",
    "    global data_index\n",
    "    assert batch_size%num_skips==0\n",
    "    assert num_skips<=2*skip_window\n",
    "    batch=np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    span=2*skip_window+1   #入队长度\n",
    "    buffer=collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):  #双向队列填入初始值\n",
    "        buffer.append(data[data_index])\n",
    "        data_index=(data_index+1)%len(data)  \n",
    "        \n",
    "    for i in range(batch_size//num_skips):  #第一次循环，i表示第几次入双向队列deque\n",
    "        for j in range(span):  #内部循环，处理deque\n",
    "            if j>skip_window:\n",
    "                batch[i*num_skips+j-1]=buffer[skip_window]\n",
    "                labels[i*num_skips+j-1,0]=buffer[j]\n",
    "            elif j==skip_window:\n",
    "                continue\n",
    "            else:\n",
    "                batch[i*num_skips+j]=buffer[skip_window]\n",
    "                labels[i*num_skips+j,0]=buffer[j]\n",
    "        buffer.append(data[data_index])  #入队一个单词，出队一个单词\n",
    "        data_index=(data_index+1)%len(data)\n",
    "    return batch,labels \n",
    "\n",
    "#开始训练\n",
    "batch_size=128   \n",
    "embedding_size=128\n",
    "skip_window=1\n",
    "num_skips=2\n",
    "num_sampled=64  #训练时用来做负样本的噪声单词的数量\n",
    "#验证数据\n",
    "valid_size=16 #抽取的验证单词数\n",
    "valid_window=100 #验证单词只从频数最高的100个单词中抽取\n",
    "valid_examples=np.random.choice(valid_window,valid_size,replace=False)#不重复在0——10l里取16个\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    948\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"nce_loss/Slice_1:0\", shape=(?, 128), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7a377ffe6acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnce_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnce_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnce_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnce_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnce_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36mnce_loss\u001b[0;34m(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name)\u001b[0m\n\u001b[1;32m   1239\u001b[0m       \u001b[0mremove_accidental_hits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_accidental_hits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mpartition_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartition_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1242\u001b[0m   sampled_losses = sigmoid_cross_entropy_with_logits(\n\u001b[1;32m   1243\u001b[0m       labels=labels, logits=logits, name=\"sampled_losses\")\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36m_compute_sampled_logits\u001b[0;34m(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[1;31m# sampled_w has shape [num_sampled, dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[1;31m# Apply X*W', which yields [batch_size, num_sampled]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m     \u001b[0msampled_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[1;31m# Retrieve the true and sampled biases, compute the true logits, and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2122\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4565\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4566\u001b[0m         \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4567\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4568\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4569\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Johnson\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    544\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 546\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs=tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels=tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "    valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "    embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size], -1, 1))   #初始化embedding vector\n",
    "    embed=tf.nn.embedding_lookup(embeddings, train_inputs) \n",
    "    \n",
    "    #用NCE loss作为优化训练的目标 \n",
    "    nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size], stddev=1.0/np.math.sqrt(embedding_size)))\n",
    "    nce_bias=tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    loss=tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_bias, embed, train_labels, num_sampled, num_classes=vocabulary_size))\n",
    "    optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "    normalized_embeddings=embeddings/norm   #除以其L2范数后得到标准化后的normalized_embeddings\n",
    "    \n",
    "    valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)    #如果输入的是64，那么对应的embedding是normalized_embeddings第64行的vector\n",
    "    similarity=tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)   #计算验证单词的嵌入向量与词汇表中所有单词的相似性\n",
    "    \n",
    "    init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d4a018794173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initialized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mavg_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "num_steps=1000001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "    avg_loss=0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs,batch_labels=generate_batch(batch_size, num_skips, skip_window)  #产生批次训练样本\n",
    "        feed_dict={train_inputs:batch_inputs,train_labels:batch_labels}   #赋值\n",
    "        _,loss_val=session.run([optimizer,loss],feed_dict=feed_dict)\n",
    "        avg_loss+=loss_val\n",
    "        if step % 2000 ==0:\n",
    "            if step>0:\n",
    "                avg_loss/=2000\n",
    "            print(\"Avg loss at step \",step,\": \",avg_loss)\n",
    "            avg_loss=0\n",
    "        if step%10000==0:\n",
    "            sim=similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word=reverse_dictionary[valid_examples[i]]  #得到验证单词\n",
    "                top_k=8  \n",
    "                nearest=(-sim[i,:]).argsort()[1:top_k+1]     #每一个valid_example相似度最高的top-k个单词\n",
    "                log_str=\"Nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word=reverse_dictionary[nearest[k]]\n",
    "                    log_str=\"%s %s,\" %(log_str,close_word)\n",
    "                print(log_str)\n",
    "    final_embedding=normalized_embeddings.eval()\n",
    "'''\n",
    "可视化Word2Vec散点图并保存\n",
    "'''\n",
    "def plot_with_labels(low_dim_embs,labels,filename):\n",
    "    assert low_dim_embs.shape[0]>=len(labels),\"more labels than embedding\"\n",
    "    plt.figure(figsize=(18,18))\n",
    "    for i,label in enumerate(labels):\n",
    "        x,y=low_dim_embs[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,xy=(x,y),xytext=(5,2),textcoords='offset points',ha='right',va='bottom')\n",
    "    plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne=TSNE(perplexity=30,n_components=2,init='pca',n_iter=5000)\n",
    "plot_number=150\n",
    "low_dim_embs=tsne.fit_transform(final_embedding[:plot_number,:])\n",
    "labels=[reverse_dictionary[i] for i in range(plot_number)]\n",
    "plot_with_labels(low_dim_embs, labels, './plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=GBK\n",
    "'''\n",
    "Created on 2017年4月5日\n",
    "\n",
    "@author: Scorpio.Lu\n",
    "'''\n",
    "import collections\n",
    "import re  \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "'''\n",
    "读取源文件,并转为list输出\n",
    "@param filename:文件名\n",
    "@return: list of words\n",
    "'''\n",
    "def read_file(filename):\n",
    "    f=open(filename,'r')\n",
    "    file_read=f.read()\n",
    "    words_=re.sub(\"[^a-zA-Z]+\", \" \",file_read).lower() #正则匹配,只留下单词，且大写改小写\n",
    "    words=list(words_.split())  #length of words:1121985\n",
    "    return words\n",
    "    \n",
    "words=read_file('哈利波特1-7英文原版.txt')\n",
    "\n",
    "\n",
    "vocabulary_size=2000  #预定义频繁单词库的长度\n",
    "count=[['UNK',-1]]    #初始化单词频数统计集合\n",
    "\n",
    "'''\n",
    "1.给@param “words”中出现过的单词做频数统计，取top 1999频数的单词放入dictionary中，以便快速查询。\n",
    "2.给哈利波特这本“单词库”@param“words”编码，出现在top 1999之外的单词，统一令其为“UNK”（未知），编号为0，并统计这些单词的数量。\n",
    "@return: 哈利波特这本书的编码data，每个单词的频数统计count，词汇表dictionary及其反转形式reverse_dictionary\n",
    "'''\n",
    "def build_dataset(words):\n",
    "    counter=collections.Counter(words).most_common(vocabulary_size-1) #length of all counter:22159 取top1999频数的单词作为vocabulary，其他的作为unknown\n",
    "    count.extend(counter)\n",
    "    #搭建dictionary\n",
    "    dictionary={}\n",
    "    for word,_ in count:\n",
    "        dictionary[word]=len(dictionary)\n",
    "    data=[]\n",
    "    #全部单词转为编号\n",
    "    #先判断这个单词是否出现在dictionary，如果是，就转成编号，如果不是，则转为编号0（代表UNK）\n",
    "    unk_count=0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index=dictionary[word]\n",
    "        else:\n",
    "            index=0\n",
    "            unk_count+=1\n",
    "        data.append(index)\n",
    "    count[0][1]=unk_count\n",
    "    reverse_dictionary=dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    return data,count,dictionary,reverse_dictionary\n",
    "\n",
    "data,count,dictionary,reverse_dictionary=build_dataset(words)   \n",
    "del words #删除原始单词列表，节约内存\n",
    "\n",
    "\n",
    "\n",
    "data_index=0\n",
    "\n",
    "\n",
    "'''\n",
    "采用Skip-Gram模式\n",
    "生成word2vec训练样本\n",
    "@param batch_size:每个批次训练多少样本\n",
    "@param num_skips: 为每个单词生成多少样本（本次实验是2个），batch_size必须是num_skips的整数倍,这样可以确保由一个目标词汇生成的样本在同一个批次中。\n",
    "@param skip_window:单词最远可以联系的距离（本次实验设为1，即目标单词只能和相邻的两个单词生成样本），2*skip_window>=num_skips\n",
    "'''\n",
    "def generate_batch(batch_size,num_skips,skip_window):\n",
    "    global data_index\n",
    "    assert batch_size%num_skips==0\n",
    "    assert num_skips<=2*skip_window\n",
    "    batch=np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels=np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    span=2*skip_window+1   #入队长度\n",
    "    buffer=collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):  #双向队列填入初始值\n",
    "        buffer.append(data[data_index])\n",
    "        data_index=(data_index+1)%len(data)  \n",
    "        \n",
    "    for i in range(batch_size//num_skips):  #第一次循环，i表示第几次入双向队列deque\n",
    "        for j in range(span):  #内部循环，处理deque\n",
    "            if j>skip_window:\n",
    "                batch[i*num_skips+j-1]=buffer[skip_window]\n",
    "                labels[i*num_skips+j-1,0]=buffer[j]\n",
    "            elif j==skip_window:\n",
    "                continue\n",
    "            else:\n",
    "                batch[i*num_skips+j]=buffer[skip_window]\n",
    "                labels[i*num_skips+j,0]=buffer[j]\n",
    "        buffer.append(data[data_index])  #入队一个单词，出队一个单词\n",
    "        data_index=(data_index+1)%len(data)\n",
    "    return batch,labels    \n",
    "\n",
    "\n",
    "#开始训练\n",
    "batch_size=128   \n",
    "embedding_size=128\n",
    "skip_window=1\n",
    "num_skips=2\n",
    "num_sampled=64  #训练时用来做负样本的噪声单词的数量\n",
    "#验证数据\n",
    "valid_size=16 #抽取的验证单词数\n",
    "valid_window=100 #验证单词只从频数最高的100个单词中抽取\n",
    "valid_examples=np.random.choice(valid_window,valid_size,replace=False)#不重复在0——10l里取16个\n",
    "\n",
    "\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs=tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels=tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "    valid_dataset=tf.constant(valid_examples,dtype=tf.int32)\n",
    "    embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size], -1, 1))   #初始化embedding vector\n",
    "    embed=tf.nn.embedding_lookup(embeddings, train_inputs) \n",
    "    \n",
    "    #用NCE loss作为优化训练的目标 \n",
    "    nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size], stddev=1.0/np.math.sqrt(embedding_size)))\n",
    "    nce_bias=tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    loss=tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_bias, embed, train_labels, num_sampled, num_classes=vocabulary_size))\n",
    "    optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "    normalized_embeddings=embeddings/norm   #除以其L2范数后得到标准化后的normalized_embeddings\n",
    "    \n",
    "    valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)    #如果输入的是64，那么对应的embedding是normalized_embeddings第64行的vector\n",
    "    similarity=tf.matmul(valid_embeddings,normalized_embeddings,transpose_b=True)   #计算验证单词的嵌入向量与词汇表中所有单词的相似性\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    \n",
    "num_steps=1000001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "    avg_loss=0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs,batch_labels=generate_batch(batch_size, num_skips, skip_window)  #产生批次训练样本\n",
    "        feed_dict={train_inputs:batch_inputs,train_labels:batch_labels}   #赋值\n",
    "        _,loss_val=session.run([optimizer,loss],feed_dict=feed_dict)\n",
    "        avg_loss+=loss_val\n",
    "        if step % 2000 ==0:\n",
    "            if step>0:\n",
    "                avg_loss/=2000\n",
    "            print(\"Avg loss at step \",step,\": \",avg_loss)\n",
    "            avg_loss=0\n",
    "        if step%10000==0:\n",
    "            sim=similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word=reverse_dictionary[valid_examples[i]]  #得到验证单词\n",
    "                top_k=8  \n",
    "                nearest=(-sim[i,:]).argsort()[1:top_k+1]     #每一个valid_example相似度最高的top-k个单词\n",
    "                log_str=\"Nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word=reverse_dictionary[nearest[k]]\n",
    "                    log_str=\"%s %s,\" %(log_str,close_word)\n",
    "                print(log_str)\n",
    "    final_embedding=normalized_embeddings.eval()\n",
    "'''\n",
    "可视化Word2Vec散点图并保存\n",
    "'''\n",
    "def plot_with_labels(low_dim_embs,labels,filename):\n",
    "    assert low_dim_embs.shape[0]>=len(labels),\"more labels than embedding\"\n",
    "    plt.figure(figsize=(18,18))\n",
    "    for i,label in enumerate(labels):\n",
    "        x,y=low_dim_embs[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,xy=(x,y),xytext=(5,2),textcoords='offset points',ha='right',va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "'''\n",
    "tsne实现降维，将原始的128维的嵌入向量降到2维\n",
    "'''\n",
    "\n",
    "tsne=TSNE(perplexity=30,n_components=2,init='pca',n_iter=5000)\n",
    "plot_number=150\n",
    "low_dim_embs=tsne.fit_transform(final_embedding[:plot_number,:])\n",
    "labels=[reverse_dictionary[i] for i in range(plot_number)]\n",
    "plot_with_labels(low_dim_embs, labels, './plot.png')\n",
    "\n",
    "       \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
