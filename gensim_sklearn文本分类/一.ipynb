{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1：文档向量化\n",
    "这部分的内容主要是由gensim来完成，\n",
    "\n",
    "主要步骤：\n",
    "1：将各文档分词，从字符串转换为单词列表\n",
    "2：统计各文档单词，生成词典（dictionary）\n",
    "3：利用词典将文档转换为词频来表示的向量，即指向量中的各值对应于词典中对应位置单词在该文档中出现次数\n",
    "4：再进行进一步处理，将词频表示的向量转换为tf-idf表示的向量\n",
    "5：由tf-idf表示的向量转换成lsi表示的向量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache H:\\TEMP\\jieba.cache\n",
      "Loading model cost 1.280 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['面对', '当前', '挑战', '，', '我们', '应该', '落实', '2030', '年', '可', '持续', '发展', '议程', '，', '促进', '包容性', '发展']\n"
     ]
    }
   ],
   "source": [
    "# 1.1文档分词及预处理\n",
    "import jieba\n",
    "\n",
    "content = \"\"\"面对当前挑战，我们应该落实2030年可持续发展议程，促进包容性发展\"\"\"\n",
    "content = list(jieba.cut(content,cut_all=False))\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2去停用词\n",
    "def convert_doc_to_wordlist(str_doc,cut_all):\n",
    "    # 分词的主要方法\n",
    "    sent_list = str_doc.split('\\n')\n",
    "    sent_list = map(rm_char, sent_list) # 去掉一些字符，例如\\u3000\n",
    "    word_2dlist = [rm_tokens(jieba.cut(part,cut_all=cut_all)) for part in sent_list] # 分词\n",
    "    word_list = sum(word_2dlist,[])\n",
    "    return word_list\n",
    "\n",
    "    \n",
    "def rm_char(text):\n",
    "    text = re.sub('\\u3000','',text)\n",
    "    return text\n",
    "\n",
    "def get_stop_words(path='./stopword.dic'):\n",
    "    # stop_words中，每行放一个停用词，以\\n分隔\n",
    "    file = open(path,'rb').read().decode('gbk').split('\\n')\n",
    "    return set(file)\n",
    "\n",
    "def rm_tokens(words): # 去掉一些停用次和数字\n",
    "    words_list = list(words)\n",
    "    stop_words = get_stop_words()\n",
    "    for i in range(words_list.__len__())[::-1]:\n",
    "        if words_list[i] in stop_words: # 去除停用词\n",
    "            words_list.pop(i)\n",
    "        elif words_list[i].isdigit():\n",
    "            words_list.pop(i)\n",
    "    return words_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '一个'),\n",
      " (1, '但是'),\n",
      " (2, '情况'),\n",
      " (3, '教育'),\n",
      " (4, '教育局'),\n",
      " (5, '治理'),\n",
      " (6, '现在'),\n",
      " (7, '这么'),\n",
      " (8, '非要'),\n",
      " (9, '不'),\n",
      " (10, '为什么'),\n",
      " (11, '什么'),\n",
      " (12, '会'),\n",
      " (13, '出现'),\n",
      " (14, '又'),\n",
      " (15, '搞'),\n",
      " (16, '明白'),\n",
      " (17, '然而'),\n",
      " (18, '词'),\n",
      " (19, '这些')]\n"
     ]
    }
   ],
   "source": [
    "# 1.3统计单词，生成词典\n",
    "#一般来说，生成词典应该在将所有文档都分玩词以后统一进行，不过对于规模特别大的数据，可以赖用变分词边统计的方法\n",
    "#将文本分批读取分词，然后用之前生成的词典加入新内容的统计结果\n",
    "from gensim import corpora,models\n",
    "import jieba\n",
    "import re\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "\n",
    "files = [\"但是现在教育局非要治理这么一个情况\",\n",
    "         \"然而又不搞明白为什么这些词会出现\"]\n",
    "\n",
    "dictionary = corpora.Dictionary()\n",
    "for file in files:\n",
    "    file = convert_doc_to_wordlist(file,cut_all=True)\n",
    "    dictionary.add_documents([file])\n",
    "    \n",
    "\n",
    "pprint(sorted(list(dictionary.items()),key=lambda x:x[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于已经存在的词典，可以使用dictionary.add_docuuments来往其中增加新的内容，当生成词典以后，你会发现词典中的词太多了，达到了几十万的数量级，因此需要去掉出现次数较少的单词，\n",
    "\n",
    "small_freq_ids = [tokenis for tokenis,docfreq in dictionary.dfs.items() if docfreq<5]\n",
    "dictionary.filter_tokens(small_freq_ids)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "# 1.4将文档转换成按词频表示的向量\n",
    "#继续沿着之前的思路走，接下来要用dictionary将文档从词语列表转化成用词频表示的向量，也就是one-hot表示的向量，所谓one-hot 就是向量\n",
    "# 中的一维对应词典的一项，如果以词频表示，则向量中该维的值即为词典中该单词在文档中出现的次数，其实这个转化很简单\n",
    "# 使用dictionary.doc2bow方法即可\n",
    "\n",
    "count = 0\n",
    "bow  = []\n",
    "for file in files:\n",
    "    count += 1\n",
    "    if count%100 == 0 :\n",
    "        print('{c} at {t}'.format(c=count, t=time.strftime('%Y-%m-%d %H:%M:%S',time.localtime())))\n",
    "    word_list = convert_doc_to_wordlist(file, cut_all=False)\n",
    "    word_bow = dictionary.doc2bow(word_list)\n",
    "    bow.append(word_bow)\n",
    "print(bow)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a2e011b921e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1.5转化成tf-idf和lsi向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m tfidf_model = models.TfidfModel(corpus=corpus,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                 dictionary=dictionary)\n\u001b[0;32m      4\u001b[0m \u001b[0mcorpus_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtfidf_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m lsi_model = models.LsiModel(corpus = corpus_tfidf, \n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# 1.5转化成tf-idf和lsi向量\n",
    "tfidf_model = models.TfidfModel(corpus=corpus,\n",
    "                                dictionary=dictionary)\n",
    "corpus_tfidf = [tfidf_model[doc] for doc in corpus]\n",
    "lsi_model = models.LsiModel(corpus = corpus_tfidf, \n",
    "                            id2word = dictionary, \n",
    "                            num_topics=50)\n",
    "corpus_lsi = [lsi_model[doc] for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2：分类问题\n",
    "在将文本向量化以后，就可以采用传统的分类方法了\n",
    "2.1从gensim到sklearn的格式转换\n",
    "一个尴尬的问题是，gensim的corpus数据格式，sklearn是无法识别的，即gensim中向量的表示形式与sklearn要求的不符\n",
    "\n",
    "在gensim中，向量是稀疏表示的\n",
    "\n",
    "所以这边只讲如何将gensim中的corpus格式转化成csr_matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "line_count = 0\n",
    "for line in lsi_corpus_total:\n",
    "    for elem in line:\n",
    "        rows.append(line_count)\n",
    "        cols.append(elem[0])\n",
    "        data.append(elem[1])\n",
    "    line_count += 1\n",
    "lsi_matrix = csr_matrix((data,(rows,cols))).toarray()\n",
    "rarray=np.random.random(size=line_count)\n",
    "train_set = []\n",
    "train_tag = []\n",
    "test_set = []\n",
    "test_tag = []\n",
    "for i in range(line_count):\n",
    "    if rarray[i]<0.8:\n",
    "        train_set.append(lsi_matrix[i,:])\n",
    "        train_tag.append(tag_list[i])\n",
    "    else:\n",
    "        test_set.append(lsi_matrix[i,:])\n",
    "        test_tag.append(tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
