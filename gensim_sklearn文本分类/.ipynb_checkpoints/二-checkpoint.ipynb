{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在有五个主要流程：\n",
    "\n",
    "生成词典\n",
    "生成tfidf向量\n",
    "生成lsi向量\n",
    "分类器参数训练\n",
    "对新文本进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os,re,time,logging\n",
    "import jieba\n",
    "import pickle as pkl\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING,\n",
    "#                     format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "#                     datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "#                     )\n",
    "\n",
    "\n",
    "class loadFolders(object):   # 迭代器\n",
    "    def __init__(self,par_path):\n",
    "        self.par_path = par_path\n",
    "    def __iter__(self):\n",
    "        for file in os.listdir(self.par_path):\n",
    "            file_abspath = os.path.join(self.par_path, file)\n",
    "            if os.path.isdir(file_abspath): # if file is a folder\n",
    "                yield file_abspath\n",
    "class loadFiles(object):\n",
    "    def __init__(self,par_path):\n",
    "        self.par_path = par_path\n",
    "    def __iter__(self):\n",
    "        folders = loadFolders(self.par_path)\n",
    "        for folder in folders:              # level directory\n",
    "            catg = folder.split(os.sep)[-1]\n",
    "            for file in os.listdir(folder):     # secondary directory\n",
    "                file_path = os.path.join(folder,file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    this_file = open(file_path,'rb')\n",
    "                    content = this_file.read().decode('utf8')\n",
    "                    yield catg,content\n",
    "                    this_file.close()\n",
    "\n",
    "def convert_doc_to_wordlist(str_doc,cut_all):\n",
    "    sent_list = str_doc.split('\\n')\n",
    "    sent_list = map(rm_char, sent_list) # 去掉一些字符，例如\\u3000\n",
    "    word_2dlist = [rm_tokens(jieba.cut(part,cut_all=cut_all)) for part in sent_list] # 分词\n",
    "    word_list = sum(word_2dlist,[])\n",
    "    return word_list\n",
    "def rm_tokens(words): # 去掉一些停用次和数字\n",
    "    words_list = list(words)\n",
    "    stop_words = get_stop_words()\n",
    "    for i in range(words_list.__len__())[::-1]:\n",
    "        if words_list[i] in stop_words: # 去除停用词\n",
    "            words_list.pop(i)\n",
    "        elif words_list[i].isdigit():\n",
    "            words_list.pop(i)\n",
    "    return words_list\n",
    "def get_stop_words(path='/home/multiangle/coding/python/PyNLP/static/stop_words.txt'):\n",
    "    file = open(path,'rb').read().decode('utf8').split('\\n')\n",
    "    return set(file)\n",
    "def rm_char(text):\n",
    "    text = re.sub('\\u3000','',text)\n",
    "    return text\n",
    "\n",
    "def svm_classify(train_set,train_tag,test_set,test_tag):\n",
    "\n",
    "    clf = svm.LinearSVC()\n",
    "    clf_res = clf.fit(train_set,train_tag)\n",
    "    train_pred  = clf_res.predict(train_set)\n",
    "    test_pred = clf_res.predict(test_set)\n",
    "\n",
    "    train_err_num, train_err_ratio = checkPred(train_tag, train_pred)\n",
    "    test_err_num, test_err_ratio  = checkPred(test_tag, test_pred)\n",
    "\n",
    "    print('=== 分类训练完毕，分类结果如下 ===')\n",
    "    print('训练集误差: {e}'.format(e=train_err_ratio))\n",
    "    print('检验集误差: {e}'.format(e=test_err_ratio))\n",
    "\n",
    "    return clf_res\n",
    "\n",
    "\n",
    "def checkPred(data_tag, data_pred):\n",
    "    if data_tag.__len__() != data_pred.__len__():\n",
    "        raise RuntimeError('The length of data tag and data pred should be the same')\n",
    "    err_count = 0\n",
    "    for i in range(data_tag.__len__()):\n",
    "        if data_tag[i]!=data_pred[i]:\n",
    "            err_count += 1\n",
    "    err_ratio = err_count / data_tag.__len__()\n",
    "    return [err_count, err_ratio]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    path_doc_root = '/media/multiangle/F/DataSet/THUCNews/THUCNewsTotal' # 根目录 即存放按类分类好的问本纪\n",
    "    path_tmp = '/media/multiangle/F/DataSet/THUCNews/tmp'  # 存放中间结果的位置\n",
    "    path_dictionary     = os.path.join(path_tmp, 'THUNews.dict')\n",
    "    path_tmp_tfidf      = os.path.join(path_tmp, 'tfidf_corpus')\n",
    "    path_tmp_lsi        = os.path.join(path_tmp, 'lsi_corpus')\n",
    "    path_tmp_lsimodel   = os.path.join(path_tmp, 'lsi_model.pkl')\n",
    "    path_tmp_predictor  = os.path.join(path_tmp, 'predictor.pkl')\n",
    "    n = 10  # n 表示抽样率， n抽1\n",
    "\n",
    "    dictionary = None\n",
    "    corpus_tfidf = None\n",
    "    corpus_lsi = None\n",
    "    lsi_model = None\n",
    "    predictor = None\n",
    "    if not os.path.exists(path_tmp):\n",
    "        os.makedirs(path_tmp)\n",
    "    # # ===================================================================\n",
    "    # # # # 第一阶段，  遍历文档，生成词典,并去掉频率较少的项\n",
    "    #       如果指定的位置没有词典，则重新生成一个。如果有，则跳过该阶段\n",
    "    if not os.path.exists(path_dictionary):\n",
    "        print('=== 未检测到有词典存在，开始遍历生成词典 ===')\n",
    "        dictionary = corpora.Dictionary()\n",
    "        files = loadFiles(path_doc_root)\n",
    "        for i,msg in enumerate(files):\n",
    "            if i%n==0:\n",
    "                catg    = msg[0]\n",
    "                file    = msg[1]\n",
    "                file = convert_doc_to_wordlist(file,cut_all=False)\n",
    "                dictionary.add_documents([file])\n",
    "                if int(i/n)%1000==0:\n",
    "                    print('{t} *** {i} \\t docs has been dealed'\n",
    "                          .format(i=i,t=time.strftime('%Y-%m-%d %H:%M:%S',time.localtime())))\n",
    "        # 去掉词典中出现次数过少的\n",
    "        small_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < 5 ]\n",
    "        dictionary.filter_tokens(small_freq_ids)\n",
    "        dictionary.compactify()\n",
    "        dictionary.save(path_dictionary)\n",
    "        print('=== 词典已经生成 ===')\n",
    "    else:\n",
    "        print('=== 检测到词典已经存在，跳过该阶段 ===')\n",
    "\n",
    "    # # ===================================================================\n",
    "    # # # # 第二阶段，  开始将文档转化成tfidf\n",
    "    if not os.path.exists(path_tmp_tfidf):\n",
    "        print('=== 未检测到有tfidf文件夹存在，开始生成tfidf向量 ===')\n",
    "        # 如果指定的位置没有tfidf文档，则生成一个。如果有，则跳过该阶段\n",
    "        if not dictionary:  # 如果跳过了第一阶段，则从指定位置读取词典\n",
    "            dictionary = corpora.Dictionary.load(path_dictionary)\n",
    "        os.makedirs(path_tmp_tfidf)\n",
    "        files = loadFiles(path_doc_root)\n",
    "        tfidf_model = models.TfidfModel(dictionary=dictionary)\n",
    "        corpus_tfidf = {}\n",
    "        for i, msg in enumerate(files):\n",
    "            if i%n==0:\n",
    "                catg    = msg[0]\n",
    "                file    = msg[1]\n",
    "                word_list = convert_doc_to_wordlist(file,cut_all=False)\n",
    "                file_bow = dictionary.doc2bow(word_list)\n",
    "                file_tfidf = tfidf_model[file_bow]\n",
    "                tmp = corpus_tfidf.get(catg,[])\n",
    "                tmp.append(file_tfidf)\n",
    "                if tmp.__len__()==1:\n",
    "                    corpus_tfidf[catg] = tmp\n",
    "            if i%10000==0:\n",
    "                print('{i} files is dealed'.format(i=i))\n",
    "        # 将tfidf中间结果储存起来\n",
    "        catgs = list(corpus_tfidf.keys())\n",
    "        for catg in catgs:\n",
    "            corpora.MmCorpus.serialize('{f}{s}{c}.mm'.format(f=path_tmp_tfidf,s=os.sep,c=catg),\n",
    "                                       corpus_tfidf.get(catg),\n",
    "                                       id2word = dictionary\n",
    "                                       )\n",
    "            print('catg {c} has been transformed into tfidf vector'.format(c=catg))\n",
    "        print('=== tfidf向量已经生成 ===')\n",
    "    else:\n",
    "        print('=== 检测到tfidf向量已经生成，跳过该阶段 ===')\n",
    "\n",
    "    # # ===================================================================\n",
    "    # # # # 第三阶段，  开始将tfidf转化成lsi\n",
    "    if not os.path.exists(path_tmp_lsi):\n",
    "        print('=== 未检测到有lsi文件夹存在，开始生成lsi向量 ===')\n",
    "        if not dictionary:\n",
    "            dictionary = corpora.Dictionary.load(path_dictionary)\n",
    "        if not corpus_tfidf: # 如果跳过了第二阶段，则从指定位置读取tfidf文档\n",
    "            print('--- 未检测到tfidf文档，开始从磁盘中读取 ---')\n",
    "            # 从对应文件夹中读取所有类别\n",
    "            files = os.listdir(path_tmp_tfidf)\n",
    "            catg_list = []\n",
    "            for file in files:\n",
    "                t = file.split('.')[0]\n",
    "                if t not in catg_list:\n",
    "                    catg_list.append(t)\n",
    "\n",
    "            # 从磁盘中读取corpus\n",
    "            corpus_tfidf = {}\n",
    "            for catg in catg_list:\n",
    "                path = '{f}{s}{c}.mm'.format(f=path_tmp_tfidf,s=os.sep,c=catg)\n",
    "                corpus = corpora.MmCorpus(path)\n",
    "                corpus_tfidf[catg] = corpus\n",
    "            print('--- tfidf文档读取完毕，开始转化成lsi向量 ---')\n",
    "\n",
    "        # 生成lsi model\n",
    "        os.makedirs(path_tmp_lsi)\n",
    "        corpus_tfidf_total = []\n",
    "        catgs = list(corpus_tfidf.keys())\n",
    "        for catg in catgs:\n",
    "            tmp = corpus_tfidf.get(catg)\n",
    "            corpus_tfidf_total += tmp\n",
    "        lsi_model = models.LsiModel(corpus = corpus_tfidf_total, id2word=dictionary, num_topics=50)\n",
    "        # 将lsi模型存储到磁盘上\n",
    "        lsi_file = open(path_tmp_lsimodel,'wb')\n",
    "        pkl.dump(lsi_model, lsi_file)\n",
    "        lsi_file.close()\n",
    "        del corpus_tfidf_total # lsi model已经生成，释放变量空间\n",
    "        print('--- lsi模型已经生成 ---')\n",
    "\n",
    "        # 生成corpus of lsi, 并逐步去掉 corpus of tfidf\n",
    "        corpus_lsi = {}\n",
    "        for catg in catgs:\n",
    "            corpu = [lsi_model[doc] for doc in corpus_tfidf.get(catg)]\n",
    "            corpus_lsi[catg] = corpu\n",
    "            corpus_tfidf.pop(catg)\n",
    "            corpora.MmCorpus.serialize('{f}{s}{c}.mm'.format(f=path_tmp_lsi,s=os.sep,c=catg),\n",
    "                                       corpu,\n",
    "                                       id2word=dictionary)\n",
    "        print('=== lsi向量已经生成 ===')\n",
    "    else:\n",
    "        print('=== 检测到lsi向量已经生成，跳过该阶段 ===')\n",
    "\n",
    "    # # ===================================================================\n",
    "    # # # # 第四阶段，  分类\n",
    "    if not os.path.exists(path_tmp_predictor):\n",
    "        print('=== 未检测到判断器存在，开始进行分类过程 ===')\n",
    "        if not corpus_lsi: # 如果跳过了第三阶段\n",
    "            print('--- 未检测到lsi文档，开始从磁盘中读取 ---')\n",
    "            files = os.listdir(path_tmp_lsi)\n",
    "            catg_list = []\n",
    "            for file in files:\n",
    "                t = file.split('.')[0]\n",
    "                if t not in catg_list:\n",
    "                    catg_list.append(t)\n",
    "            # 从磁盘中读取corpus\n",
    "            corpus_lsi = {}\n",
    "            for catg in catg_list:\n",
    "                path = '{f}{s}{c}.mm'.format(f=path_tmp_lsi,s=os.sep,c=catg)\n",
    "                corpus = corpora.MmCorpus(path)\n",
    "                corpus_lsi[catg] = corpus\n",
    "            print('--- lsi文档读取完毕，开始进行分类 ---')\n",
    "\n",
    "        tag_list = []\n",
    "        doc_num_list = []\n",
    "        corpus_lsi_total = []\n",
    "        catg_list = []\n",
    "        files = os.listdir(path_tmp_lsi)\n",
    "        for file in files:\n",
    "            t = file.split('.')[0]\n",
    "            if t not in catg_list:\n",
    "                catg_list.append(t)\n",
    "        for count,catg in enumerate(catg_list):\n",
    "            tmp = corpus_lsi[catg]\n",
    "            tag_list += [count]*tmp.__len__()\n",
    "            doc_num_list.append(tmp.__len__())\n",
    "            corpus_lsi_total += tmp\n",
    "            corpus_lsi.pop(catg)\n",
    "\n",
    "        # 将gensim中的mm表示转化成numpy矩阵表示\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        line_count = 0\n",
    "        for line in corpus_lsi_total:\n",
    "            for elem in line:\n",
    "                rows.append(line_count)\n",
    "                cols.append(elem[0])\n",
    "                data.append(elem[1])\n",
    "            line_count += 1\n",
    "        lsi_matrix = csr_matrix((data,(rows,cols))).toarray()\n",
    "        # 生成训练集和测试集\n",
    "        rarray=np.random.random(size=line_count)\n",
    "        train_set = []\n",
    "        train_tag = []\n",
    "        test_set = []\n",
    "        test_tag = []\n",
    "        for i in range(line_count):\n",
    "            if rarray[i]<0.8:\n",
    "                train_set.append(lsi_matrix[i,:])\n",
    "                train_tag.append(tag_list[i])\n",
    "            else:\n",
    "                test_set.append(lsi_matrix[i,:])\n",
    "                test_tag.append(tag_list[i])\n",
    "\n",
    "        # 生成分类器\n",
    "        predictor = svm_classify(train_set,train_tag,test_set,test_tag)\n",
    "        x = open(path_tmp_predictor,'wb')\n",
    "        pkl.dump(predictor, x)\n",
    "        x.close()\n",
    "    else:\n",
    "        print('=== 检测到分类器已经生成，跳过该阶段 ===')\n",
    "\n",
    "    # # ===================================================================\n",
    "    # # # # 第五阶段，  对新文本进行判断\n",
    "    if not dictionary:\n",
    "        dictionary = corpora.Dictionary.load(path_dictionary)\n",
    "    if not lsi_model:\n",
    "        lsi_file = open(path_tmp_lsimodel,'rb')\n",
    "        lsi_model = pkl.load(lsi_file)\n",
    "        lsi_file.close()\n",
    "    if not predictor:\n",
    "        x = open(path_tmp_predictor,'rb')\n",
    "        predictor = pkl.load(x)\n",
    "        x.close()\n",
    "    files = os.listdir(path_tmp_lsi)\n",
    "    catg_list = []\n",
    "    for file in files:\n",
    "        t = file.split('.')[0]\n",
    "        if t not in catg_list:\n",
    "            catg_list.append(t)\n",
    "    demo_doc = \"\"\"\n",
    "这次大选让两党的精英都摸不着头脑。以媒体专家的传统观点来看，要选总统首先要避免失言，避免说出一些“offensive”的话。希拉里，罗姆尼，都是按这个方法操作的。罗姆尼上次的47%言论是在一个私人场合被偷录下来的，不是他有意公开发表的。今年希拉里更是从来没有召开过新闻发布会。\n",
    "川普这种肆无忌惮的发言方式，在传统观点看来等于自杀。\n",
    "\"\"\"\n",
    "    print(\"原文本内容为：\")\n",
    "    print(demo_doc)\n",
    "    demo_doc = list(jieba.cut(demo_doc,cut_all=False))\n",
    "    demo_bow = dictionary.doc2bow(demo_doc)\n",
    "    tfidf_model = models.TfidfModel(dictionary=dictionary)\n",
    "    demo_tfidf = tfidf_model[demo_bow]\n",
    "    demo_lsi = lsi_model[demo_tfidf]\n",
    "    data = []\n",
    "    cols = []\n",
    "    rows = []\n",
    "    for item in demo_lsi:\n",
    "        data.append(item[1])\n",
    "        cols.append(item[0])\n",
    "        rows.append(0)\n",
    "    demo_matrix = csr_matrix((data,(rows,cols))).toarray()\n",
    "    x = predictor.predict(demo_matrix)\n",
    "    print('分类结果为：{x}'.format(x=catg_list[x[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
